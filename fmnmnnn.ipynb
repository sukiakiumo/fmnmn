{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_functionl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def variable_summaries(var, name,dorelu):\n",
    "  with tf.name_scope(\"summaries\"):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.scalar_summary('mean/' + name, mean)\n",
    "    if dorelu==1:\n",
    "        var = tf.nn.relu6(var)\n",
    "        \n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "    tf.scalar_summary('sttdev/' + name, stddev)\n",
    "    tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "    tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "    tf.scalar_summary('sparsity/'+ name, tf.nn.zero_fraction(var))\n",
    "    tf.histogram_summary(name, var)\n",
    "    \n",
    "def variable_summaries_list(varDict):\n",
    "  for name,var in varDict.items():\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "      mean = tf.reduce_mean(var)\n",
    "      tf.scalar_summary(name + 'mean/', mean)\n",
    "      tf.scalar_summary(name + 'sttdev/',  tf.sqrt(tf.reduce_sum(tf.square(var - mean))))\n",
    "      tf.scalar_summary(name + 'max/', tf.reduce_max(var))\n",
    "      tf.scalar_summary(name + 'min/', tf.reduce_min(var))\n",
    "      tf.scalar_summary(name + 'sparsity/', tf.nn.zero_fraction(var))\n",
    "      tf.histogram_summary(name, var)\n",
    "\n",
    "def testAccuracy(arrayValue,stopthresh=0.5):\n",
    "  #   print('V1, V2, Diff, Std = (%.03f, %.03f, %.03f, %.03f)' % \n",
    "  #         (arrayValue[-1], np.mean(arrayValue[-7:-2]),np.abs(arrayValue[-1]-np.mean(arrayValue[-7:-2])), np.std(arrayValue[-7:-2])))\n",
    "  notChanging = np.abs(arrayValue[-1]-np.mean(arrayValue[-7:-2])) < stopthresh*np.std(arrayValue[-7:-2])\n",
    "  slopeNegative = np.mean(np.gradient(arrayValue[-7:-1]))<0\n",
    "  return notChanging|slopeNegative\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5       , -0.5       , -0.5       , ..., -0.49607843,\n",
       "        -0.48823529, -0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       , ...,  0.49215686,\n",
       "         0.5       ,  0.39411765],\n",
       "       [-0.5       , -0.5       , -0.5       , ...,  0.5       ,\n",
       "         0.49607843,  0.31176472],\n",
       "       ..., \n",
       "       [ 0.24509804,  0.5       ,  0.49215686, ..., -0.5       ,\n",
       "        -0.5       , -0.5       ],\n",
       "       [-0.5       , -0.5       , -0.5       , ..., -0.44117647,\n",
       "        -0.5       , -0.5       ],\n",
       "       [-0.5       , -0.5       , -0.48823529, ..., -0.5       ,\n",
       "        -0.5       , -0.5       ]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_1d(data, w1,w2,w3,b1,b2,b3, dropoutList):\n",
    "     # Hidden Layer Training computation.\n",
    "  vals = tf.matmul(data, w1) + b1\n",
    "  hidden1 = tf.nn.dropout(tf.nn.tanh(vals),dropoutList[0])            \n",
    "  vals2 = tf.matmul(hidden1,w2)+b2\n",
    "  hidden2 = tf.nn.dropout(tf.nn.tanh(vals2),dropoutList[1])            \n",
    "  vals3 = tf.matmul(hidden2,w3)+b3\n",
    "#  hidden3 = tf.nn.relu(vals3)\n",
    "#  if dropout:\n",
    "#        hidden3 = tf.nn.relu(vals3)\n",
    "#  vals4 = tf.matmul(tf.nn.dropout(hidden3,.25),weights4)+biases4\n",
    "  return vals3\n",
    "\n",
    "def exploreData(datasetDict, paramDict):\n",
    "  doWrite = 0; # this isn't working\n",
    "  tf.reset_default_graph()\n",
    "  tf.Graph().as_default()\n",
    "  graph = tf.Graph()\n",
    "  learnRate = paramDict['learnRate']\n",
    "  with graph.as_default():\n",
    "    # With Priors\n",
    "    # The datas\n",
    "    dropoutList = [1,1,1];\n",
    "    betalist = [5e-4,5e-4,5e-4]\n",
    "    tf_trainDataset = tf.constant(datasetDict['trainDataset'])\n",
    "    tf_trainLabels = tf.constant(datasetDict['trainLabels'])\n",
    "    train_subset = len(datasetDict['trainLabels'])\n",
    "\n",
    "    tf_trainDatasetRed = tf.constant(datasetDict['trainDatasetRed'])\n",
    "    tf_trainLabelsRed = tf.constant(datasetDict['trainLabelsRed'])\n",
    "    tf_validDataset = tf.constant(datasetDict['validDataset'])\n",
    "    tf_validLabels = tf.constant(datasetDict['validLabels'])\n",
    "    tf_testDataset = tf.constant(datasetDict['testDataset'])\n",
    "    tf_testLabels = tf.constant(datasetDict['testLabels'])\n",
    "    \n",
    "    num_neurons = [image_size * image_size, 512, 256, num_labels]\n",
    "    # Variables.\n",
    "    #Initialize variables\n",
    "    layer_name = 'layer1_wprior'\n",
    "    \n",
    "     # Variables.\n",
    "    w11 = tf.Variable(tf.random_normal([num_neurons[0], num_neurons[1]], 0, 2.0/np.sqrt(num_neurons[0]+num_neurons[1])))\n",
    "    b11 = tf.Variable(tf.zeros([num_neurons[1]]))\n",
    "    w12 = tf.Variable(tf.random_normal([num_neurons[1], num_neurons[2]], 0, 2.0/np.sqrt(num_neurons[1]+num_neurons[2])))\n",
    "    b12 = tf.Variable(tf.zeros([num_neurons[2]]))\n",
    "    w13 = tf.Variable(tf.random_normal([num_neurons[2], num_neurons[3]], 0, 2.0/np.sqrt(num_neurons[2]+num_neurons[3])))\n",
    "    b13 = tf.Variable(tf.zeros([num_neurons[3]]))\n",
    "    \n",
    "    # All priors are given the same initial value.\n",
    "    priors = tf.Variable(tf.ones([train_subset])*np.float(paramDict['priorInit']),name='priors')\n",
    "#     priors = tf.add(priors,tf.random_normal(train_subset,0,.05))\n",
    "    # Training computation.\n",
    "    # Find the classification estimation 'z' value\n",
    "    logits = model_1d(tf_trainDataset, w11,w12,w13,b11,b12,b13, dropoutList)\n",
    "    # Calculate the cross-entropy loss. \n",
    "#     loss1 = tf.reduce_mean(tf.mul(\n",
    "#               tf.nn.relu6(priors*6),tf.nn.softmax_cross_entropy_with_logits(logits, tf_trainLabels)\n",
    "#                                   )\n",
    "#                          )/tf.reduce_mean(tf.nn.relu6(priors*6))\n",
    "    loss1 = tf.reduce_mean(tf.mul(\n",
    "              tf.nn.sigmoid(priors),tf.nn.softmax_cross_entropy_with_logits(logits, tf_trainLabels)\n",
    "                                  )\n",
    "                         )/tf.reduce_mean(tf.nn.sigmoid(priors))+betalist[0]*tf.nn.l2_loss(w11)+betalist[1]*tf.nn.l2_loss(w12)+betalist[2]*tf.nn.l2_loss(w13)\n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer1 = tf.train.GradientDescentOptimizer(learnRate).minimize(loss1)\n",
    "    # Predictions for the training, validation. \n",
    "    # Do not use the priors to perform classification!\n",
    "    train_prediction1 = tf.nn.softmax(logits)\n",
    "    valid_prediction1 = tf.nn.softmax(model_1d(tf_validDataset, w11,w12,w13,b11,b12,b13, [1,1,1]))\n",
    "    test_prediction1 = tf.nn.softmax(model_1d(tf_testDataset, w11,w12,w13,b11,b12,b13, [1,1,1]))\n",
    "\n",
    "    # WITHOUT PRIORS\n",
    "    layer_name = 'layer1_normal'\n",
    "    w21 = tf.Variable(w11.initialized_value())\n",
    "    b21 = tf.Variable(b11.initialized_value())\n",
    "    w22 = tf.Variable(w12.initialized_value())\n",
    "    b22 = tf.Variable(b12.initialized_value())\n",
    "    w23 = tf.Variable(w13.initialized_value())\n",
    "    b23 = tf.Variable(b13.initialized_value())\n",
    "    \n",
    "    # Training computation.\n",
    "    # Find the classification estimation 'z' value\n",
    "    logits2 = model_1d(tf_trainDataset, w21,w22,w23,b21,b22,b23, dropoutList)\n",
    "\n",
    "    # Calculate the cross-entropy loss. \n",
    "    loss2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits2, tf_trainLabels))+betalist[0]*tf.nn.l2_loss(w21)+betalist[1]*tf.nn.l2_loss(w22)+betalist[2]*tf.nn.l2_loss(w23)\n",
    "\n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer2 = tf.train.GradientDescentOptimizer(learnRate).minimize(loss2)\n",
    "    # Predictions for the training, validation. \n",
    "    train_prediction2 = tf.nn.softmax(logits2)\n",
    "    valid_prediction2 = tf.nn.softmax(model_1d(tf_validDataset, w21,w22,w23,b21,b22,b23, [1,1,1]))\n",
    "    test_prediction2 = tf.nn.softmax(model_1d(tf_testDataset, w21,w22,w23,b21,b22,b23, [1,1,1]))                                  \n",
    "\n",
    "    # WITHOUT PRIORS REDUCED DATA\n",
    "    layer_name = 'layer1_normal_reducedDatas'\n",
    "    w31 = tf.Variable(w11.initialized_value())\n",
    "    b31 = tf.Variable(b11.initialized_value())\n",
    "    w32 = tf.Variable(w12.initialized_value())\n",
    "    b32 = tf.Variable(b12.initialized_value())\n",
    "    w33 = tf.Variable(w13.initialized_value())\n",
    "    b33 = tf.Variable(b13.initialized_value())\n",
    "    # Training computation.\n",
    "    logits3 = model_1d(tf_trainDatasetRed, w21,w22,w23,b21,b22,b23, dropoutList)\n",
    "\n",
    "    # Calculate the cross-entropy loss. \n",
    "    loss3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits3, tf_trainLabelsRed))+100*betalist[0]*tf.nn.l2_loss(w31)+100*betalist[1]*tf.nn.l2_loss(w32)+100*betalist[2]*tf.nn.l2_loss(w33)   \n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer3 = tf.train.GradientDescentOptimizer(learnRate).minimize(loss3)\n",
    "    # Predictions for the training, validation. \n",
    "    train_prediction3 = tf.nn.softmax(logits3)\n",
    "    valid_prediction3 = tf.nn.softmax(model_1d(tf_validDataset, w31,w32,w33,b31,b32,b33, [1,1,1]))\n",
    "    test_prediction3 = tf.nn.softmax(model_1d(tf_testDataset, w31,w32,w33,b31,b32,b33, [1,1,1]))  \n",
    "\n",
    "  train_acc_priors = []\n",
    "  valid_acc_priors = []\n",
    "  train_acc_normal = []\n",
    "  valid_acc_normal = []\n",
    "  train_acc_normal_red = []\n",
    "  valid_acc_normal_red = []\n",
    "#   num_steps = 40000\n",
    "  num_steps = 1000\n",
    "  train1 = True\n",
    "  train2 = True\n",
    "  train3 = True\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    # Initialize\n",
    "    tf.initialize_all_variables().run()    \n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    train_dir = 'tmp'\n",
    "    summary_writer = tf.train.SummaryWriter(train_dir, session.graph)  \n",
    "    print('Initialized')\n",
    "    modelPriorLastStep = num_steps\n",
    "    modelNormalLastStep = num_steps\n",
    "    modelNormalRedLastStep = num_steps\n",
    "\n",
    "    for step in range(num_steps):\n",
    "      # Run the computations.\n",
    "      if train1:\n",
    "        # With priors\n",
    "        _, lvl1, predictions1 = session.run([optimizer1, loss1, train_prediction1])\n",
    "      if train2:\n",
    "        # Without priors\n",
    "        _, lvl2, predictions2 = session.run([optimizer2, loss2, train_prediction2])\n",
    "      if train3:\n",
    "        # Without priors\n",
    "        _, lvl3, predictions3 = session.run([optimizer3, loss3, train_prediction3])\n",
    "\n",
    "      if (step % 10 == 0):\n",
    "        if doWrite:\n",
    "          summary_str = session.run(summary_op)\n",
    "          summary_writer.add_summary(summary_str, step)  \n",
    "\n",
    "        acc1 = accuracy(predictions1, datasetDict['trainLabels'])\n",
    "        acc2 =  accuracy(valid_prediction1.eval(), datasetDict['validLabels'])\n",
    "        train_acc_priors.append(acc1)\n",
    "        valid_acc_priors.append(acc2)\n",
    "        print('With prior. Loss at step %d: %f . Training accuracy: %.1f%% . Validation accuracy: %.1f%%' % (step, lvl1 , acc1, acc2))\n",
    "\n",
    "        acc1 = accuracy(predictions2, datasetDict['trainLabels'])\n",
    "        acc2 =  accuracy(valid_prediction2.eval(), datasetDict['validLabels'])\n",
    "        train_acc_normal.append(acc1)\n",
    "        valid_acc_normal.append(acc2)\n",
    "        print('Nope prior. Loss at step %d: %f . Training accuracy: %.1f%% . Validation accuracy: %.1f%%' % (step, lvl2 , acc1, acc2))\n",
    "\n",
    "        acc1 = accuracy(predictions3, trainLabelsRed)\n",
    "        acc2 =  accuracy(valid_prediction3.eval(), datasetDict['validLabels'])\n",
    "        train_acc_normal_red.append(acc1)\n",
    "        valid_acc_normal_red.append(acc2)\n",
    "        print('Nope prior. Loss at step %d: %f . Training accuracy: %.1f%% . Validation accuracy: %.1f%%' % (step, lvl3 , acc1, acc2))\n",
    "      stopthresh = 0.5;\n",
    "      if step > 70:\n",
    "        if testAccuracy(train_acc_priors) and train1:\n",
    "          train1 = False\n",
    "          modelPriorLastStep = step\n",
    "          print('No longer training w/ priors')\n",
    "        if testAccuracy(train_acc_normal) and train2:\n",
    "          train2 = False\n",
    "          modelNormalLastStep = step\n",
    "          print('No longer training w/ normal method')\n",
    "        if testAccuracy(train_acc_normal_red) and train3:\n",
    "          train3 = False\n",
    "          modelNormalRedLastStep = step\n",
    "          print('No longer training w/ normal + reduced data')\n",
    "      if train1==False and train2==False and train3==False:\n",
    "        print('Stopping Training at step ', step)\n",
    "        break;\n",
    "\n",
    "    tp1 = accuracy(test_prediction1.eval(), dDict['testLabels']);\n",
    "    tp2 = accuracy(test_prediction2.eval(), dDict['testLabels']);\n",
    "    tp3 = accuracy(test_prediction3.eval(), dDict['testLabels'])\n",
    "    print('Test accuracy with prior: %.1f%%' % tp1)\n",
    "    print('Test accuracy no prior: %.1f%%' % tp2)\n",
    "    print('Test accuracy no prior on small data: %.1f%%' % tp3)\n",
    "    accDict = {'trainAcc_priors': train_acc_priors, 'validAcc_prior': valid_acc_priors,\n",
    "              'trainAcc_normal': train_acc_normal, 'validAcc_normal': valid_acc_normal,\n",
    "              'trainAcc_normalRed': train_acc_normal_red, 'validAcc_normalRed': valid_acc_normal_red, \n",
    "              'testAcc_prior': tp1, 'testAcc_normal': tp2, 'testAcc_normalRed': tp3,\n",
    "            'modelPriorLastStep': modelPriorLastStep, 'modelNormalLastStep': modelNormalLastStep,'modelNormalRedLastStep': modelNormalRedLastStep}\n",
    "    \n",
    "  return accDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeling 0.0 of data or 0 / 10000 examples\n",
      "Initialized\n",
      "With prior. Loss at step 0: 3.339994 . Training accuracy: 10.7% . Validation accuracy: 55.3%\n",
      "Nope prior. Loss at step 0: 3.339957 . Training accuracy: 10.7% . Validation accuracy: 63.0%\n",
      "Nope prior. Loss at step 0: 50.945671 . Training accuracy: 55.6% . Validation accuracy: 10.7%\n",
      "With prior. Loss at step 10: 1.551033 . Training accuracy: 71.0% . Validation accuracy: 70.0%\n",
      "Nope prior. Loss at step 10: 1.155748 . Training accuracy: 80.8% . Validation accuracy: 79.2%\n",
      "Nope prior. Loss at step 10: 30.198553 . Training accuracy: 80.8% . Validation accuracy: 10.5%\n",
      "With prior. Loss at step 20: 1.163072 . Training accuracy: 80.6% . Validation accuracy: 78.9%\n",
      "Nope prior. Loss at step 20: 1.013592 . Training accuracy: 85.5% . Validation accuracy: 82.8%\n",
      "Nope prior. Loss at step 20: 18.306084 . Training accuracy: 85.7% . Validation accuracy: 10.6%\n",
      "With prior. Loss at step 30: 1.045035 . Training accuracy: 84.4% . Validation accuracy: 82.2%\n",
      "Nope prior. Loss at step 30: 0.962054 . Training accuracy: 86.9% . Validation accuracy: 83.0%\n",
      "Nope prior. Loss at step 30: 11.192002 . Training accuracy: 86.9% . Validation accuracy: 10.4%\n",
      "With prior. Loss at step 40: 1.009573 . Training accuracy: 85.5% . Validation accuracy: 82.8%\n",
      "Nope prior. Loss at step 40: 0.921678 . Training accuracy: 88.1% . Validation accuracy: 83.1%\n",
      "Nope prior. Loss at step 40: 6.898692 . Training accuracy: 87.9% . Validation accuracy: 10.3%\n",
      "With prior. Loss at step 50: 0.989697 . Training accuracy: 85.9% . Validation accuracy: 82.5%\n",
      "Nope prior. Loss at step 50: 0.920925 . Training accuracy: 87.9% . Validation accuracy: 82.6%\n",
      "Nope prior. Loss at step 50: 4.332902 . Training accuracy: 87.6% . Validation accuracy: 10.4%\n",
      "With prior. Loss at step 60: 0.958652 . Training accuracy: 86.7% . Validation accuracy: 83.2%\n",
      "Nope prior. Loss at step 60: 0.911009 . Training accuracy: 88.4% . Validation accuracy: 83.3%\n",
      "Nope prior. Loss at step 60: 2.722479 . Training accuracy: 89.7% . Validation accuracy: 10.4%\n",
      "With prior. Loss at step 70: 0.936204 . Training accuracy: 87.3% . Validation accuracy: 83.2%\n",
      "Nope prior. Loss at step 70: 0.834516 . Training accuracy: 90.4% . Validation accuracy: 83.2%\n",
      "Nope prior. Loss at step 70: 1.768471 . Training accuracy: 90.1% . Validation accuracy: 10.4%\n",
      "With prior. Loss at step 80: 0.914333 . Training accuracy: 88.0% . Validation accuracy: 83.0%\n",
      "Nope prior. Loss at step 80: 0.799377 . Training accuracy: 91.5% . Validation accuracy: 83.1%\n",
      "Nope prior. Loss at step 80: 1.169790 . Training accuracy: 91.5% . Validation accuracy: 10.4%\n",
      "With prior. Loss at step 90: 0.897036 . Training accuracy: 88.5% . Validation accuracy: 83.1%\n",
      "Nope prior. Loss at step 90: 0.784228 . Training accuracy: 91.8% . Validation accuracy: 83.1%\n",
      "Nope prior. Loss at step 90: 0.819622 . Training accuracy: 91.8% . Validation accuracy: 10.4%\n",
      "With prior. Loss at step 100: 0.901225 . Training accuracy: 88.1% . Validation accuracy: 82.7%\n",
      "Nope prior. Loss at step 100: 0.759015 . Training accuracy: 93.0% . Validation accuracy: 83.4%\n",
      "Nope prior. Loss at step 100: 0.590374 . Training accuracy: 92.7% . Validation accuracy: 10.4%\n",
      "With prior. Loss at step 110: 0.863264 . Training accuracy: 89.0% . Validation accuracy: 83.3%\n",
      "Nope prior. Loss at step 110: 0.758999 . Training accuracy: 92.7% . Validation accuracy: 81.5%\n",
      "Nope prior. Loss at step 110: 0.478369 . Training accuracy: 92.2% . Validation accuracy: 10.4%\n",
      "With prior. Loss at step 120: 0.958486 . Training accuracy: 86.0% . Validation accuracy: 82.0%\n",
      "Nope prior. Loss at step 120: 0.754853 . Training accuracy: 92.8% . Validation accuracy: 82.6%\n",
      "Nope prior. Loss at step 120: 0.395617 . Training accuracy: 92.2% . Validation accuracy: 10.4%\n",
      "With prior. Loss at step 130: 0.845587 . Training accuracy: 89.6% . Validation accuracy: 82.8%\n",
      "Nope prior. Loss at step 130: 0.691004 . Training accuracy: 95.4% . Validation accuracy: 83.8%\n",
      "Nope prior. Loss at step 130: 0.274012 . Training accuracy: 95.3% . Validation accuracy: 10.4%\n",
      "No longer training w/ priors\n",
      "With prior. Loss at step 140: 0.845587 . Training accuracy: 89.6% . Validation accuracy: 82.8%\n",
      "Nope prior. Loss at step 140: 0.678642 . Training accuracy: 95.7% . Validation accuracy: 83.6%\n",
      "Nope prior. Loss at step 140: 0.233638 . Training accuracy: 95.9% . Validation accuracy: 10.4%\n",
      "With prior. Loss at step 150: 0.845587 . Training accuracy: 89.6% . Validation accuracy: 82.8%\n",
      "Nope prior. Loss at step 150: 0.660951 . Training accuracy: 96.3% . Validation accuracy: 83.8%\n",
      "Nope prior. Loss at step 150: 0.201633 . Training accuracy: 96.4% . Validation accuracy: 10.4%\n",
      "With prior. Loss at step 160: 0.845587 . Training accuracy: 89.6% . Validation accuracy: 82.8%\n",
      "Nope prior. Loss at step 160: 0.649230 . Training accuracy: 96.7% . Validation accuracy: 83.4%\n",
      "Nope prior. Loss at step 160: 0.182087 . Training accuracy: 96.6% . Validation accuracy: 10.4%\n",
      "With prior. Loss at step 170: 0.845587 . Training accuracy: 89.6% . Validation accuracy: 82.8%\n",
      "Nope prior. Loss at step 170: 0.669511 . Training accuracy: 96.2% . Validation accuracy: 82.5%\n",
      "Nope prior. Loss at step 170: 0.202019 . Training accuracy: 94.9% . Validation accuracy: 10.4%\n",
      "No longer training w/ normal + reduced data\n",
      "With prior. Loss at step 180: 0.845587 . Training accuracy: 89.6% . Validation accuracy: 82.8%\n",
      "Nope prior. Loss at step 180: 0.627738 . Training accuracy: 97.4% . Validation accuracy: 83.7%\n",
      "Nope prior. Loss at step 180: 0.202019 . Training accuracy: 94.9% . Validation accuracy: 10.4%\n",
      "With prior. Loss at step 190: 0.845587 . Training accuracy: 89.6% . Validation accuracy: 82.8%\n",
      "Nope prior. Loss at step 190: 0.716545 . Training accuracy: 93.8% . Validation accuracy: 83.1%\n",
      "Nope prior. Loss at step 190: 0.202019 . Training accuracy: 94.9% . Validation accuracy: 10.4%\n",
      "With prior. Loss at step 200: 0.845587 . Training accuracy: 89.6% . Validation accuracy: 82.8%\n",
      "Nope prior. Loss at step 200: 0.614725 . Training accuracy: 97.8% . Validation accuracy: 84.0%\n",
      "Nope prior. Loss at step 200: 0.202019 . Training accuracy: 94.9% . Validation accuracy: 10.4%\n",
      "No longer training w/ normal method\n",
      "Stopping Training at step  200\n",
      "Test accuracy with prior: 89.5%\n",
      "Test accuracy no prior: 90.3%\n",
      "Test accuracy no prior on small data: 10.4%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dDict = dict()\n",
    "dDict['validDataset'] = valid_dataset\n",
    "dDict['validLabels'] = valid_labels\n",
    "dDict['testDataset'] = test_dataset\n",
    "dDict['testLabels'] = test_labels\n",
    "\n",
    "\n",
    "train_subset = 10000\n",
    "train_dataset2 = train_dataset[:train_subset, :]\n",
    "train_labels2 = train_labels[:train_subset]\n",
    "dDict['trainDataset'] = train_dataset2\n",
    "\n",
    "numTest = 5;\n",
    "fracBadList = np.linspace(0,.8, num=numTest) \n",
    "fracBadList = [0.0]\n",
    "outList = []\n",
    "pDict = dict()\n",
    "for i, fracBad in enumerate(fracBadList):\n",
    "  pDict['priorInit'] = (1-fracBad)*.9\n",
    "  pDict['learnRate'] = 0.5\n",
    "  nBad = np.int(fracBad*train_labels2.shape[0])\n",
    "  print('Mislabeling %0.1f of data or %i / %i examples' % (fracBad, nBad, train_subset))\n",
    "  rangeNess = np.arange(train_labels2.shape[0])\n",
    "  np.random.shuffle(rangeNess)\n",
    "  randomBreak = rangeNess[:nBad]\n",
    "  trainL = train_labels2\n",
    "  for i, rb in enumerate(randomBreak):\n",
    "    trainL[rb][:] = trainL[rb][np.random.permutation(10)];\n",
    "  trainDatasetRed = np.delete(train_dataset2,randomBreak,0)\n",
    "  trainLabelsRed = np.delete(train_labels2,randomBreak,0)\n",
    "\n",
    "  dDict['trainLabels'] = trainL\n",
    "  dDict['trainDatasetRed'] = trainDatasetRed\n",
    "  dDict['trainLabelsRed'] = trainLabelsRed  \n",
    "  ar = exploreData(dDict, pDict)\n",
    "  outList.append(ar)\n",
    "  with open('results_nnn_0.0dropout_0.0.dat', 'wb') as outfile:\n",
    "    pickle.dump(outList, outfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outList[0].items()\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('test.dat', 'wb') as outfile:\n",
    "    pickle.dump(outList, outfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10c9af828>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c9af7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ar['trainAcc_priors'])\n",
    "plt.plot(ar['trainAcc_normal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ar['trainAcc_priors'])\n",
    "plt.plot(ar['trainAcc_normal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['testAcc_normalRed', 'validAcc_normalRed', 'trainAcc_normal', 'modelPriorLastStep', 'testAcc_prior', 'trainAcc_normalRed', 'trainAcc_priors', 'modelNormalLastStep', 'modelNormalRedLastStep', 'testAcc_normal', 'validAcc_prior', 'validAcc_normal'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using 50% dropout, here were the results:\n",
    "'''\n",
    "Stopping Training at step  320\n",
    "Test accuracy with prior: 89.7%\n",
    "Test accuracy no prior: 89.9%\n",
    "Test accuracy no prior on small data: 8.6%\n",
    "'''\n",
    "\n",
    "When using 0% dropout, here were the results:\n",
    "'''\n",
    "Stopping Training at step  200\n",
    "Test accuracy with prior: 89.5%\n",
    "Test accuracy no prior: 90.3%\n",
    "Test accuracy no prior on small data: 10.4%\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

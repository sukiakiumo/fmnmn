{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def variable_summaries(var, name,dorelu):\n",
    "  with tf.name_scope(\"summaries\"):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.scalar_summary('mean/' + name, mean)\n",
    "    if dorelu==1:\n",
    "        var = tf.nn.relu6(var)\n",
    "        \n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "    tf.scalar_summary('sttdev/' + name, stddev)\n",
    "    tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "    tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "    tf.scalar_summary('sparsity/'+ name, tf.nn.zero_fraction(var))\n",
    "    tf.histogram_summary(name, var)\n",
    "    \n",
    "def variable_summaries_list(varDict):\n",
    "  for name,var in varDict.items():\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "      mean = tf.reduce_mean(var)\n",
    "      tf.scalar_summary(name + 'mean/', mean)\n",
    "      tf.scalar_summary(name + 'sttdev/',  tf.sqrt(tf.reduce_sum(tf.square(var - mean))))\n",
    "      tf.scalar_summary(name + 'max/', tf.reduce_max(var))\n",
    "      tf.scalar_summary(name + 'min/', tf.reduce_min(var))\n",
    "      tf.scalar_summary(name + 'sparsity/', tf.nn.zero_fraction(var))\n",
    "      tf.histogram_summary(name, var)\n",
    "\n",
    "def testAccuracy(arrayValue,stopthresh=0.5):\n",
    "  #   print('V1, V2, Diff, Std = (%.03f, %.03f, %.03f, %.03f)' % \n",
    "  #         (arrayValue[-1], np.mean(arrayValue[-7:-2]),np.abs(arrayValue[-1]-np.mean(arrayValue[-7:-2])), np.std(arrayValue[-7:-2])))\n",
    "  notChanging = np.abs(arrayValue[-1]-np.mean(arrayValue[-7:-2])) < stopthresh*np.std(arrayValue[-7:-2])\n",
    "  slopeNegative = np.mean(np.gradient(arrayValue[-7:-1]))<0\n",
    "  return notChanging|slopeNegative\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exploreData(datasetDict, paramDict):\n",
    "  doWrite = 0; # this isn't working\n",
    "  tf.reset_default_graph()\n",
    "  tf.Graph().as_default()\n",
    "  graph = tf.Graph()\n",
    "  learnRate = paramDict['learnRate']\n",
    "  with graph.as_default():\n",
    "    # With Priors\n",
    "    # The datas\n",
    "    \n",
    "    tf_trainDataset = tf.constant(datasetDict['trainDataset'])\n",
    "    tf_trainLabels = tf.constant(datasetDict['trainLabels'])\n",
    "    train_subset = len(datasetDict['trainLabels'])\n",
    "\n",
    "    tf_trainDatasetRed = tf.constant(datasetDict['trainDatasetRed'])\n",
    "    tf_trainLabelsRed = tf.constant(datasetDict['trainLabelsRed'])\n",
    "    tf_validDataset = tf.constant(datasetDict['validDataset'])\n",
    "    tf_validLabels = tf.constant(datasetDict['validLabels'])\n",
    "    tf_testDataset = tf.constant(datasetDict['testDataset'])\n",
    "    tf_testLabels = tf.constant(datasetDict['testLabels'])\n",
    "\n",
    "    # Variables.\n",
    "    #with tf.name_scope('hidden') as scope:\n",
    "    layer_name = 'layer1_wprior'\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]),name='weights')\n",
    "    biases = tf.Variable(tf.zeros([num_labels]),name='biases')\n",
    "    # All priors are given the same prior: 90%\n",
    "    priors = tf.Variable(tf.ones([train_subset])*np.float(paramDict['priorInit']),name='priors')\n",
    "    # Training computation.\n",
    "    # Find the classification estimation 'z' value\n",
    "    logits = tf.matmul(tf_trainDataset, weights) + biases\n",
    "    # Calculate the cross-entropy loss. \n",
    "    loss1 = tf.reduce_mean(tf.mul(\n",
    "              tf.nn.relu6(priors*6),tf.nn.softmax_cross_entropy_with_logits(logits, tf_trainLabels)\n",
    "                                  )\n",
    "                         )/tf.reduce_mean(tf.nn.relu6(priors*6))\n",
    "    if doWrite:\n",
    "      varDict1={layer_name+'weights': weights}\n",
    "      varDict1[layer_name+'biases'] = biases\n",
    "      varDict1[layer_name+'priors'] = tf.nn.relu6(priors*6)  \n",
    "      variable_summaries_list(varDict1)\n",
    "      tf.scalar_summary('loss/' + layer_name, loss1)\n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer1 = tf.train.GradientDescentOptimizer(learnRate).minimize(loss1)\n",
    "    # Predictions for the training, validation. \n",
    "    # Do not use the priors to perform classification!\n",
    "    train_prediction1 = tf.nn.softmax(logits)\n",
    "    valid_prediction1 = tf.nn.softmax(tf.matmul(tf_validDataset, weights) + biases)\n",
    "    test_prediction1 = tf.nn.softmax(tf.matmul(tf_testDataset, weights) + biases)\n",
    "\n",
    "    # WITHOUT PRIORS\n",
    "    layer_name = 'layer1_normal'\n",
    "    weights2 = tf.Variable(weights.initialized_value(),name='weights2')\n",
    "    biases2 = tf.Variable(biases.initialized_value(),name='biases2')\n",
    "    # Training computation.\n",
    "    # Find the classification estimation 'z' value\n",
    "    logits2 = tf.matmul(tf_trainDataset, weights2) + biases2\n",
    "    # Calculate the cross-entropy loss. \n",
    "    loss2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits2, tf_trainLabels))\n",
    "    if doWrite:\n",
    "      varDict2={layer_name+'weights': weights2}\n",
    "      varDict2[layer_name+'biases'] = biases2\n",
    "      variable_summaries_list(varDict2)\n",
    "      tf.scalar_summary('loss/' + layer_name, loss2)    \n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer2 = tf.train.GradientDescentOptimizer(learnRate).minimize(loss2)\n",
    "    # Predictions for the training, validation. \n",
    "    train_prediction2 = tf.nn.softmax(logits2)\n",
    "    valid_prediction2 = tf.nn.softmax(tf.matmul(tf_validDataset, weights2) + biases2)\n",
    "    # Predictions for the test data set. \n",
    "    test_prediction2 = tf.nn.softmax(tf.matmul(tf_testDataset, weights2) + biases2)                                    \n",
    "\n",
    "    # WITHOUT PRIORS REDUCED DATA\n",
    "    layer_name = 'layer1_normal_reducedDatas'\n",
    "    weights3 = tf.Variable(weights.initialized_value(),name='weights3')\n",
    "    biases3 = tf.Variable(biases.initialized_value(),name='biases3')\n",
    "    # Training computation.\n",
    "    # Find the classification estimation 'z' value\n",
    "    logits3 = tf.matmul(tf_trainDatasetRed, weights3) + biases3\n",
    "    # Calculate the cross-entropy loss. \n",
    "    loss3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits3, tf_trainLabelsRed))\n",
    "    if doWrite:\n",
    "      varDict3={layer_name+'weights': weights3}\n",
    "      varDict3[layer_name+'biases'] = biases3\n",
    "      variable_summaries_list(varDict3)\n",
    "      tf.scalar_summary('loss/' + layer_name, loss3)    \n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer3 = tf.train.GradientDescentOptimizer(learnRate).minimize(loss3)\n",
    "    # Predictions for the training, validation. \n",
    "    train_prediction3 = tf.nn.softmax(logits3)\n",
    "    valid_prediction3 = tf.nn.softmax(tf.matmul(tf_validDataset, weights2) + biases3)\n",
    "    # Predictions for the test data set. \n",
    "    test_prediction3 = tf.nn.softmax(tf.matmul(tf_testDataset, weights3) + biases3)                                    \n",
    "\n",
    "  train_acc_priors = []\n",
    "  valid_acc_priors = []\n",
    "  train_acc_normal = []\n",
    "  valid_acc_normal = []\n",
    "  train_acc_normal_red = []\n",
    "  valid_acc_normal_red = []\n",
    "#   num_steps = 40000\n",
    "  num_steps = 40000\n",
    "  train1 = True\n",
    "  train2 = True\n",
    "  train3 = True\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    # Initialize\n",
    "    tf.initialize_all_variables().run()    \n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    train_dir = 'tmp'\n",
    "    summary_writer = tf.train.SummaryWriter(train_dir, session.graph)  \n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "      # Run the computations.\n",
    "      if train1:\n",
    "        # With priors\n",
    "        _, lvl1, predictions1 = session.run([optimizer1, loss1, train_prediction1])\n",
    "      if train2:\n",
    "        # Without priors\n",
    "        _, lvl2, predictions2 = session.run([optimizer2, loss2, train_prediction2])\n",
    "      if train3:\n",
    "        # Without priors\n",
    "        _, lvl3, predictions3 = session.run([optimizer3, loss3, train_prediction3])\n",
    "\n",
    "      if (step % 100 == 0):\n",
    "        if doWrite:\n",
    "          summary_str = session.run(summary_op)\n",
    "          summary_writer.add_summary(summary_str, step)  \n",
    "\n",
    "        acc1 = accuracy(predictions1, datasetDict['trainLabels'])\n",
    "        acc2 =  accuracy(valid_prediction1.eval(), datasetDict['validLabels'])\n",
    "        train_acc_priors.append(acc1)\n",
    "        valid_acc_priors.append(acc2)\n",
    "        print('With prior. Loss at step %d: %f . Training accuracy: %.1f%% . Validation accuracy: %.1f%%' % (step, lvl1 , acc1, acc2))\n",
    "\n",
    "        acc1 = accuracy(predictions2, datasetDict['trainLabels'])\n",
    "        acc2 =  accuracy(valid_prediction2.eval(), datasetDict['validLabels'])\n",
    "        train_acc_normal.append(acc1)\n",
    "        valid_acc_normal.append(acc2)\n",
    "        print('Nope prior. Loss at step %d: %f . Training accuracy: %.1f%% . Validation accuracy: %.1f%%' % (step, lvl2 , acc1, acc2))\n",
    "\n",
    "        acc1 = accuracy(predictions3, trainLabelsRed)\n",
    "        acc2 =  accuracy(valid_prediction3.eval(), datasetDict['validLabels'])\n",
    "        train_acc_normal_red.append(acc1)\n",
    "        valid_acc_normal_red.append(acc2)\n",
    "        print('Nope prior. Loss at step %d: %f . Training accuracy: %.1f%% . Validation accuracy: %.1f%%' % (step, lvl3 , acc1, acc2))\n",
    "      stopthresh = 0.5;\n",
    "      if step > 1000:\n",
    "        if testAccuracy(train_acc_priors) and train1:\n",
    "          train1 = False\n",
    "          modelPriorLastStep = step\n",
    "          print('No longer training w/ priors')\n",
    "        if testAccuracy(train_acc_normal) and train2:\n",
    "          train2 = False\n",
    "          modelNormalLastStep = step\n",
    "          print('No longer training w/ normal method')\n",
    "        if testAccuracy(train_acc_normal_red) and train3:\n",
    "          train3 = False\n",
    "          modelNormalRedLastStep = step\n",
    "          print('No longer training w/ normal + reduced data')\n",
    "      if train1==False and train2==False and train3==False:\n",
    "        print('Stopping Training at step ', step)\n",
    "        break;\n",
    "#       modelPriorLastStep = step\n",
    "#       modelNormalLastStep = step\n",
    "#       modelNormalRedLastStep = step\n",
    "    tp1 = accuracy(test_prediction1.eval(), dDict['testLabels']);\n",
    "    tp2 = accuracy(test_prediction2.eval(), dDict['testLabels']);\n",
    "    tp3 = accuracy(test_prediction3.eval(), dDict['testLabels'])\n",
    "    print('Test accuracy with prior: %.1f%%' % tp1)\n",
    "    print('Test accuracy no prior: %.1f%%' % tp2)\n",
    "    print('Test accuracy no prior on small data: %.1f%%' % tp3)\n",
    "    accDict = {'trainAcc_priors': train_acc_priors, 'validAcc_prior': valid_acc_priors,\n",
    "              'trainAcc_normal': train_acc_normal, 'validAcc_normal': valid_acc_normal,\n",
    "              'trainAcc_normalRed': train_acc_normal_red, 'validAcc_normalRed': valid_acc_normal_red, \n",
    "              'testAcc_prior': tp1, 'testAcc_normal': tp2, 'testAcc_normalRed': tp3,\n",
    "            'modelPriorLastStep': modelPriorLastStep, 'modelNormalLastStep': modelNormalLastStep,'modelNormalRedLastStep': modelNormalRedLastStep}\n",
    "    \n",
    "  return accDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeling 0.9 of data or 9000 / 10000 examples\n",
      "Initialized\n",
      "With prior. Loss at step 0: 16.537668 . Training accuracy: 10.0% . Validation accuracy: 7.8%\n",
      "Nope prior. Loss at step 0: 16.537682 . Training accuracy: 10.0% . Validation accuracy: 7.8%\n",
      "Nope prior. Loss at step 0: 17.777157 . Training accuracy: 7.5% . Validation accuracy: 7.8%\n",
      "With prior. Loss at step 100: 7.154669 . Training accuracy: 11.3% . Validation accuracy: 15.8%\n",
      "Nope prior. Loss at step 100: 7.379275 . Training accuracy: 11.2% . Validation accuracy: 15.9%\n",
      "Nope prior. Loss at step 100: 2.616482 . Training accuracy: 67.8% . Validation accuracy: 15.8%\n",
      "With prior. Loss at step 200: 5.715145 . Training accuracy: 11.7% . Validation accuracy: 17.0%\n",
      "Nope prior. Loss at step 200: 6.008849 . Training accuracy: 11.7% . Validation accuracy: 16.9%\n",
      "Nope prior. Loss at step 200: 1.724699 . Training accuracy: 74.8% . Validation accuracy: 17.0%\n",
      "With prior. Loss at step 300: 4.959508 . Training accuracy: 12.1% . Validation accuracy: 17.5%\n",
      "Nope prior. Loss at step 300: 5.298488 . Training accuracy: 11.9% . Validation accuracy: 17.3%\n",
      "Nope prior. Loss at step 300: 1.258378 . Training accuracy: 79.2% . Validation accuracy: 17.4%\n",
      "With prior. Loss at step 400: 4.462003 . Training accuracy: 12.3% . Validation accuracy: 18.0%\n",
      "Nope prior. Loss at step 400: 4.836740 . Training accuracy: 12.1% . Validation accuracy: 17.4%\n",
      "Nope prior. Loss at step 400: 0.965451 . Training accuracy: 82.5% . Validation accuracy: 17.3%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dDict = dict()\n",
    "dDict['validDataset'] = valid_dataset\n",
    "dDict['validLabels'] = valid_labels\n",
    "dDict['testDataset'] = test_dataset\n",
    "dDict['testLabels'] = test_labels\n",
    "\n",
    "\n",
    "train_subset = 10000\n",
    "train_dataset2 = train_dataset[:train_subset, :]\n",
    "train_labels2 = train_labels[:train_subset]\n",
    "dDict['trainDataset'] = train_dataset2\n",
    "\n",
    "numTest = 5;\n",
    "fracBadList = np.linspace(0,.8, num=numTest) \n",
    "fracBadList = [0.9]\n",
    "outList = []\n",
    "pDict = dict()\n",
    "for i, fracBad in enumerate(fracBadList):\n",
    "  pDict['priorInit'] = 0.5#(1-fracBad)*.99\n",
    "  pDict['learnRate'] = 0.2\n",
    "  nBad = np.int(fracBad*train_labels2.shape[0])\n",
    "  print('Mislabeling %0.1f of data or %i / %i examples' % (fracBad, nBad, train_subset))\n",
    "  rangeNess = np.arange(train_labels2.shape[0])\n",
    "  np.random.shuffle(rangeNess)\n",
    "  randomBreak = rangeNess[:nBad]\n",
    "  trainL = train_labels2\n",
    "  for i, rb in enumerate(randomBreak):\n",
    "    trainL[rb][:] = trainL[rb][np.random.permutation(10)];\n",
    "  trainDatasetRed = np.delete(train_dataset2,randomBreak,0)\n",
    "  trainLabelsRed = np.delete(train_labels2,randomBreak,0)\n",
    "\n",
    "  dDict['trainLabels'] = trainL\n",
    "  dDict['trainDatasetRed'] = trainDatasetRed\n",
    "  dDict['trainLabelsRed'] = trainLabelsRed  \n",
    "  ar = exploreData(dDict, pDict)\n",
    "  outList.append(ar)\n",
    "  with open('results_long_0.9.dat', 'wb') as outfile:\n",
    "    pickle.dump(outList, outfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outList[0].items()\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Results with 60% removed after doing the full list!?\n",
    "With prior. Loss at step 21200: 0.688023 . Training accuracy: 40.0% . Validation accuracy: 11.1%\n",
    "Nope prior. Loss at step 21200: 1.960189 . Training accuracy: 30.4% . Validation accuracy: 11.2%\n",
    "Nope prior. Loss at step 21200: 1.319424 . Training accuracy: 56.7% . Validation accuracy: 10.8%\n",
    "No longer training w/ normal + reduced data\n",
    "Stopping Training at step  21200\n",
    "Test accuracy with prior: 10.8%\n",
    "Test accuracy no prior: 11.3%\n",
    "Test accuracy no prior on small data: 11.5%\n",
    "#When I reset all code this is what I get\n",
    "With prior. Loss at step 14600: 0.124448 . Training accuracy: 44.9% . Validation accuracy: 67.4%\n",
    "Nope prior. Loss at step 14600: 1.687886 . Training accuracy: 45.2% . Validation accuracy: 56.6%\n",
    "Nope prior. Loss at step 14600: 0.065873 . Training accuracy: 99.5% . Validation accuracy: 24.2%\n",
    "No longer training w/ normal method\n",
    "Stopping Training at step  14600\n",
    "Test accuracy with prior: 74.8%\n",
    "Test accuracy no prior: 63.2%\n",
    "Test accuracy no prior on small data: 81.1%\n",
    "\n",
    "What the F!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('test.dat', 'wb') as outfile:\n",
    "    pickle.dump(outList, outfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "fracBad = 0.2;\n",
    "nBad = np.int(fracBad*train_labels.shape[0])\n",
    "rangeNess = np.arange(train_labels.shape[0])\n",
    "np.random.shuffle(rangeNess)\n",
    "randomBreak = rangeNess[:nBad]\n",
    "\n",
    "for i, rb in enumerate(randomBreak):\n",
    "    train_labels[rb][:] = train_labels[rb][np.random.permutation(10)];\n",
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "doWrite = 0;\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  # With Priors\n",
    "  # The datas\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  #with tf.name_scope('hidden') as scope:\n",
    "  layer_name = 'layer1_wprior'\n",
    "  weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]),name='weights')\n",
    "  biases = tf.Variable(tf.zeros([num_labels]),name='biases')\n",
    "  # All priors are given the same prior: 90%\n",
    "  priors = tf.Variable(tf.ones([train_subset])*0.9,name='priors')\n",
    "  # Training computation.\n",
    "  # Find the classification estimation 'z' value\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  # Calculate the cross-entropy loss. \n",
    "  loss1 = tf.reduce_mean(tf.mul(\n",
    "            tf.nn.relu6(priors*6),tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)\n",
    "                                )\n",
    "                       )/tf.reduce_mean(tf.nn.relu6(priors*6))\n",
    "  if doWrite:\n",
    "    varDict1={layer_name+'weights': weights}\n",
    "    varDict1[layer_name+'biases'] = biases\n",
    "    varDict1[layer_name+'priors'] = tf.nn.relu6(priors*6)  \n",
    "    variable_summaries_list(varDict)\n",
    "    tf.scalar_summary('loss/' + layer_name, loss)\n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer1 = tf.train.GradientDescentOptimizer(0.5).minimize(loss1)\n",
    "  # Predictions for the training, validation. \n",
    "  # Do not use the priors to perform classification!\n",
    "  train_prediction1 = tf.nn.softmax(logits)\n",
    "  valid_prediction1 = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction1 = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "                                   \n",
    "  # WITHOUT PRIORS\n",
    "  layer_name = 'layer1_normal'\n",
    "  weights2 = tf.Variable(weights.initialized_value(),name='weights2')\n",
    "  biases2 = tf.Variable(biases.initialized_value(),name='biases2')\n",
    "  # Training computation.\n",
    "  # Find the classification estimation 'z' value\n",
    "  logits2 = tf.matmul(tf_train_dataset, weights2) + biases2\n",
    "  # Calculate the cross-entropy loss. \n",
    "  loss2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits2, tf_train_labels))\n",
    "  if doWrite:\n",
    "    varDict2={layer_name+'weights': weights2}\n",
    "    varDict2[layer_name+'biases'] = biases2\n",
    "    variable_summaries_list(varDict2)\n",
    "    tf.scalar_summary('loss/' + layer_name, loss2)    \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer2 = tf.train.GradientDescentOptimizer(0.5).minimize(loss2)\n",
    "  # Predictions for the training, validation. \n",
    "  train_prediction2 = tf.nn.softmax(logits2)\n",
    "  valid_prediction2 = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights2) + biases2)\n",
    "  # Predictions for the test data set. \n",
    "  test_prediction2 = tf.nn.softmax(tf.matmul(tf_test_dataset, weights2) + biases2)                                    \n",
    "\n",
    "  # WITHOUT PRIORS REDUCED DATA\n",
    "  layer_name = 'layer1_normal_reducedDatas'\n",
    "  weights3 = tf.Variable(weights.initialized_value(),name='weights3')\n",
    "  biases3 = tf.Variable(biases.initialized_value(),name='biases3')\n",
    "  # Training computation.\n",
    "  # Find the classification estimation 'z' value\n",
    "  logits3 = tf.matmul(tf_train_dataset, weights3) + biases3\n",
    "  # Calculate the cross-entropy loss. \n",
    "  loss3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits3, tf_train_labels))\n",
    "  if doWrite:\n",
    "    varDict3={layer_name+'weights': weights3}\n",
    "    varDict3[layer_name+'biases'] = biases3\n",
    "    variable_summaries_list(varDict3)\n",
    "    tf.scalar_summary('loss/' + layer_name, loss3)    \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer3 = tf.train.GradientDescentOptimizer(0.5).minimize(loss3)\n",
    "  # Predictions for the training, validation. \n",
    "  train_prediction3 = tf.nn.softmax(logits3)\n",
    "  valid_prediction3 = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights2) + biases3)\n",
    "  # Predictions for the test data set. \n",
    "  test_prediction3 = tf.nn.softmax(tf.matmul(tf_test_dataset, weights3) + biases3)                                    \n",
    "\n",
    "train_acc_priors = []\n",
    "valid_acc_priors = []\n",
    "train_acc_normal = []\n",
    "valid_acc_normal = []\n",
    "train_acc_normal_red = []\n",
    "valid_acc_normal_red = []\n",
    "num_steps = 5000\n",
    "\n",
    "train1 = True\n",
    "train2 = True\n",
    "train3 = True\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # Initialize\n",
    "  tf.initialize_all_variables().run()    \n",
    "  summary_op = tf.merge_all_summaries()\n",
    "  train_dir = 'tmp'\n",
    "  summary_writer = tf.train.SummaryWriter(train_dir, session.graph)  \n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations.\n",
    "    if train1:\n",
    "      # With priors\n",
    "      _, lvl1, predictions1 = session.run([optimizer1, loss1, train_prediction1])\n",
    "    if train2:\n",
    "      # Without priors\n",
    "      _, lvl2, predictions2 = session.run([optimizer2, loss2, train_prediction2])\n",
    "    if train3:\n",
    "      # Without priors\n",
    "      _, lvl3, predictions3 = session.run([optimizer3, loss3, train_prediction3])\n",
    "    \n",
    "    if (step % 100 == 0):\n",
    "      if doWrite:\n",
    "        summary_str = session.run(summary_op)\n",
    "        summary_writer.add_summary(summary_str, step)  \n",
    "\n",
    "      acc1 = accuracy(predictions1, train_labels[:train_subset, :])\n",
    "      acc2 =  accuracy(valid_prediction1.eval(), valid_labels)\n",
    "      train_acc_priors.append(acc1)\n",
    "      valid_acc_priors.append(acc2)\n",
    "      print('With prior. Loss at step %d: %f . Training accuracy: %.1f%% . Validation accuracy: %.1f%%' % (step, lvl1 , acc1, acc2))\n",
    "\n",
    "      acc1 = accuracy(predictions2, train_labels[:train_subset, :])\n",
    "      acc2 =  accuracy(valid_prediction2.eval(), valid_labels)\n",
    "      train_acc_normal.append(acc1)\n",
    "      valid_acc_normal.append(acc2)\n",
    "      print('Nope prior. Loss at step %d: %f . Training accuracy: %.1f%% . Validation accuracy: %.1f%%' % (step, lvl2 , acc1, acc2))\n",
    "\n",
    "      acc1 = accuracy(predictions3, train_labels[:train_subset, :])\n",
    "      acc2 =  accuracy(valid_prediction3.eval(), valid_labels)\n",
    "      train_acc_normal_red.append(acc1)\n",
    "      valid_acc_normal_red.append(acc2)\n",
    "      print('Nope prior. Loss at step %d: %f . Training accuracy: %.1f%% . Validation accuracy: %.1f%%' % (step, lvl3 , acc1, acc2))\n",
    "    stopthresh = 0.5;\n",
    "    if step > 1000:\n",
    "      if testAccuracy(train_acc_priors):\n",
    "        train1 = False\n",
    "        print('No longer training w/ priors')\n",
    "      if testAccuracy(train_acc_normal):\n",
    "        train2 = False\n",
    "        print('No longer training w/ normal method')\n",
    "      if testAccuracy(train_acc_normal):\n",
    "        train3 = False\n",
    "        print('No longer training w/ normal + reduced data')\n",
    "    if train1==False & train2==False & train3==False:\n",
    "      print('Stopping Training at step %d', step)\n",
    "      break;\n",
    "  print('Test accuracy with prior: %.1f%%' % accuracy(test_prediction1.eval(), test_labels))\n",
    "  print('Test accuracy no prior: %.1f%%' % accuracy(test_prediction2.eval(), test_labels))\n",
    "  print('Test accuracy no prior on small data: %.1f%%' % accuracy(test_prediction3.eval(), test_labels))\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
